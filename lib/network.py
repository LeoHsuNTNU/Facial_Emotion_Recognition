# -*- coding: utf-8 -*-
"""network.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XOlHwYTqx-1OZS61tXgASdFXRP8CbVQg
"""

import tensorflow as tf
import numpy as np

class conv2d(tf.keras.layers.Layer):
  def __init__(self, out_channel, kernel_size, strides, padding='SAME', dilations=None):
    self.out_channel = out_channel
    self.kernel_size = kernel_size
    self.strides = strides
    self.padding = padding
    self.dilations = dilations
    super(conv2d, self).__init__()
  
  def build(self, input_shape):
    kernel_shape = tf.TensorShape((self.kernel_size[0], self.kernel_size[1], input_shape[-1], self.out_channel))
    self.conv_weight = self.add_weight(name='conv_weight',
                                      shape=kernel_shape,
                                      initializer=tf.initializers.he_normal(),
                                      trainable=True)
    
    self.conv_bias = self.add_weight(name='conv_bias',
                                     shape=(self.out_channel),
                                     initializer=tf.zeros_initializer(),
                                     trainable=True)
  
  def call(self, inputs):
    conv = tf.nn.conv2d(input=inputs,
                        filters=self.conv_weight,
                        strides=self.strides,
                        padding=self.padding,
                        dilations=self.dilations,
                        name='conv')
    conv = tf.nn.bias_add(conv, self.conv_bias)
    return conv

class fully_connected(tf.keras.layers.Layer):
  def __init__(self, out_channel):
    self.out_channel = out_channel
    super(fully_connected, self).__init__()
  
  def build(self, input_shape):
    self.fully_weight = self.add_weight(name='fully_weight',
                                        shape=[input_shape[-1], self.out_channel],
                                        initializer=tf.keras.initializers.TruncatedNormal(),
                                        trainable=True)
    
    self.fully_bias = self.add_weight(name='fully_bias',
                                      shape=[self.out_channel],
                                      initializer=tf.keras.initializers.TruncatedNormal(),
                                      trainable=True)
    
  def call(self, inputs):
    full = tf.add(tf.matmul(inputs, self.fully_weight), self.fully_bias, name = 'fully')
    return full

def flatten(inputs):
  input_shape = tf.shape(inputs)
  return tf.reshape(inputs, [input_shape[0], input_shape[1] * input_shape[2] * input_shape[3]])

class batch_norm(tf.keras.layers.Layer):
  def __init__(self, epsilon=0.001, decay=0.9):
    """
    Create the object for batch normalization layer.
    Args:
        epsilon: the variance epsilon - a small float number to avoid dividing by 0
        decay: the moving average decay
    """
    self.epsilon = epsilon
    self.decay = decay
    super(batch_norm, self).__init__()

  def build(self, input_shape):
    """
    Take the input shape, creating the trainable variables: gamma and beta and not trainable variables: moving_avg and
    moving_var
    Args:
       x_shape: the input tensor shape.
       scope: the scope name
    """
    self.shape = input_shape
    # gamma: a trainable scale factor
    self.gamma = self.add_weight(name='gamma',
                                  shape=[self.shape[-1]],
                                  initializer=tf.constant_initializer(1.0),
                                  trainable=True)
        
    # beta: a trainable shift value 
    self.beta = self.add_weight(name='beta',
                                shape=[self.shape[-1]],
                                initializer=tf.constant_initializer(0.0),
                                trainable=True)
  
    self.moving_avg = self.add_weight(name='moving_avg',
                                      shape=[self.shape[-1]],
                                      initializer=tf.constant_initializer(0.0),
                                      trainable=False)
        
    self.moving_var = self.add_weight(name='moving_var',
                                      shape=[self.shape[-1]],
                                      initializer=tf.constant_initializer(1.0),
                                      trainable=False)

  def call(self, inputs, is_training):
    """
    Performs a batch normalization calcualtion.
    Args:
      is_training: The network status. True for training, False for inference.
    Returns:
        The result of a batch normalization layer
    """
    self.x = inputs
    if is_training:
      # tf.nn.moments == Calculate the mean and the variance of the tensor x
      avg, var = tf.nn.moments(self.x, np.arange(len(self.shape)-1), keepdims=True)
      avg = tf.reshape(avg, [avg.shape.as_list()[-1]])
      var = tf.reshape(var, [var.shape.as_list()[-1]])
      update_moving_avg = self.moving_avg.assign(self.moving_avg*self.decay+avg*(1-self.decay))
      update_moving_var = self.moving_var.assign(self.moving_var*self.decay+var*(1-self.decay))
      control_inputs = [update_moving_avg, update_moving_var]

      with tf.control_dependencies(control_inputs):
        output = tf.nn.batch_normalization(self.x, avg, var, self.beta, self.gamma, self.epsilon)
    
    else:
      avg = self.moving_avg
      var = self.moving_var
      output = tf.nn.batch_normalization(self.x, avg, var, self.beta, self.gamma, self.epsilon)
    
    return output